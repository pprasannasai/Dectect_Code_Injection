{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVhwgJjZQt1w"
      },
      "source": [
        "!pip install nltk\n",
        "!pip install gensim\n",
        "!pip install -U -q PyDrive\n",
        "!pip install keras-tuner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUXbLlcO_eoO"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skeysUYfAPE0"
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ww7PB3SsQ988"
      },
      "source": [
        "downloaded = drive.CreateFile({'id':'1jkfp6FnOrhhDzpVeWQrZxJhr3w_n8DMX'})\n",
        "downloaded.GetContentFile('keywords.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfuu4ogzRHo1"
      },
      "source": [
        "downloaded = drive.CreateFile({'id':'1aji9cGzB0edut2UVqQC_tkewtSAQcf6p'}) # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('sample1.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVDvHruoRSCO",
        "outputId": "1721cc90-e425-413b-dc3d-9b750cf4f706",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(action = 'ignore')\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "#  Reads ‘alice.txt’ file\n",
        "sample = open('keywords.txt',\"r\")\n",
        "s = sample.read()\n",
        "\n",
        "# Replaces escape character with space\n",
        "f = s.replace(\"\\n\", \" \")\n",
        "\n",
        "keywords = []\n",
        "\n",
        "# iterate through each sentence in the file\n",
        "for i in sent_tokenize(f):\n",
        "    temp = []\n",
        "\n",
        "    # tokenize the sentence into words\n",
        "    for j in word_tokenize(i):\n",
        "        temp.append(j.lower())\n",
        "\n",
        "    keywords.append(temp)\n",
        "\n",
        "# Create CBOW model\n",
        "model = gensim.models.Word2Vec(keywords, min_count = 1,\n",
        "                              size = 100, window = 5)\n",
        "\n",
        "#model = gensim.models.Word2Vec(keywords, min_count = 1, size = 100,\n",
        " #                                            window = 5, sg = 1)\n",
        "\n",
        "words = list(model.wv.vocab)\n",
        "embeddings_index = {}\n",
        "for word in words:\n",
        "  embeddings_index[word]=model.wv[word]\n",
        "\n",
        "#for attr, value in embeddings_index.items():\n",
        "  #print(str(attr)+\" \"+str(value))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHBrisHi8DLp"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import keras\n",
        "from keras import optimizers\n",
        "from keras import backend as K\n",
        "from keras import regularizers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten\n",
        "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "from keras.utils import plot_model\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tqdm import tqdm\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import os, re, csv, math, codecs\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "np.random.seed(0)\n",
        "\n",
        "MAX_NB_WORDS = 100000\n",
        "tokenizer = RegexpTokenizer(r'\\w+')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb3vegFZ7Xf5"
      },
      "source": [
        "data = pd.read_csv(\"sample1.csv\")\n",
        "ngrams_list = data['ngram feature vector'].tolist()\n",
        "label_list  = data['class'].tolist()\n",
        "one_hot_labels = keras.utils.to_categorical(label_list, num_classes=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPP1Wa1ZNaGE",
        "outputId": "1c8fff04-8328-4060-8651-c7ff2696c764",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "processed_list = []\n",
        "for sentence in tqdm(ngrams_list):\n",
        "    tokens = sentence.lower().split()\n",
        "    filtered = [word for word in tokens if word not in keywords]\n",
        "    processed_list.append(\" \".join(filtered))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 92/92 [00:00<00:00, 231063.45it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRSYLXnFAWLF",
        "outputId": "21a3a5fb-9c69-4052-b659-643514e29e90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df = pd.DataFrame()\n",
        "df[\"ngrams\"] = processed_list\n",
        "df['doc_len'] = df['ngrams'].apply(lambda words: len(words.split(\" \")))\n",
        "max_seq_len = np.round(df['doc_len'].mean() + df['doc_len'].std()).astype(int)\n",
        "max_seq_len"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8PP2DtoA0iS"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(processed_list, one_hot_labels, test_size=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "or53ZCOxA6xf",
        "outputId": "45578eb3-a484-4799-d338-536b518bd6c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(x_train + x_test)\n",
        "word_seq_train = tokenizer.texts_to_sequences(x_train)\n",
        "word_seq_test = tokenizer.texts_to_sequences(x_test)\n",
        "word_index = tokenizer.word_index\n",
        "print(\"dictionary size: \", len(word_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dictionary size:  15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Adg5wkQ5BCo_"
      },
      "source": [
        "num_tokens = [len(tokens) for tokens in word_seq_train + word_seq_test]\n",
        "num_tokens = np.array(num_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcYqILI_BG_B"
      },
      "source": [
        "word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\n",
        "word_seq_test = sequence.pad_sequences(word_seq_test, maxlen=max_seq_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6KgTNrsBL9g"
      },
      "source": [
        "word_seq_test = word_seq_test[:-1]\n",
        "y_test = y_test[:-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9I9-7vxQBQEP"
      },
      "source": [
        "#training params\n",
        "batch_size = 4\n",
        "num_epochs = 8\n",
        "\n",
        "#model parameters\n",
        "num_filters = 64\n",
        "embed_dim = 100\n",
        "weight_decay = 1e-4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljvMkFhkFAib"
      },
      "source": [
        "embedding_matrix = np.zeros((len(word_index)+1, 100))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index[word]\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[index] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfdQJJ8MJMYW",
        "outputId": "4423273d-8e6e-462f-de78-b3f87f97717b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(len(word_index)+1, embed_dim, weights=[embedding_matrix], input_length=max_seq_len, trainable=False))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(num_filters, 5, padding='same', activation='relu', strides=1))\n",
        "model.add(MaxPooling1D(2))\n",
        "model.add(Conv1D(num_filters, 5, activation='relu', padding='same'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 4, 100)            1600      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 4, 100)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 4, 64)             32064     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 2, 64)             0         \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 2, 64)             20544     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_2 (Glob (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 2)                 66        \n",
            "=================================================================\n",
            "Total params: 56,354\n",
            "Trainable params: 54,754\n",
            "Non-trainable params: 1,600\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmKDLx6CLMZU",
        "outputId": "565e804a-f923-4861-ae82-9cfbffa121b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "model.fit(word_seq_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 289us/step - loss: 0.6974 - acc: 0.4531\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe3400c3ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyeVxksYS8Vx"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD0eua-OQTQS",
        "outputId": "73cb52bc-f9e1-45ba-b2b2-3b09811712f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model.evaluate(word_seq_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r27/27 [==============================] - 0s 3ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6968455910682678, 0.5555555820465088]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    }
  ]
}
